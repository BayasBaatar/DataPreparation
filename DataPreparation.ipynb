{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Bayas\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import tflearn\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from parameters import Parameters\n",
    "from pyspark.sql import SparkSession\n",
    "from functions import Functions\n",
    "from DataCleansing import DataCleansing\n",
    "from Training import Training\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Functions:\n",
    "\n",
    "    # Reads JSON file and saves \"config\" as jsonData\n",
    "    @staticmethod\n",
    "    def readJSON():\n",
    "        with open('data/model_data.json') as f:\n",
    "            data = json.load(f)\n",
    "            jsonData = data['config']\n",
    "        return jsonData\n",
    "\n",
    "    # Loads CSV file and applies configuration\n",
    "    @staticmethod\n",
    "    def loadCsvFile(spark, file_name):\n",
    "        print(\"Loading CSV Data\")\n",
    "        spark_df = spark.read.format(\"csv\") \\\n",
    "            .option(\"delimiter\", \",\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"header\", True) \\\n",
    "            .load(Parameters.CSV_PATH + file_name)\n",
    "\n",
    "        # Transform the Apache dataframe into Pandas dataframe\n",
    "        df_data = spark_df.select('*').toPandas()\n",
    "\n",
    "        # Reads data points from JSON\n",
    "        json_data = Functions.readJSON()\n",
    "        man = json_data['list_of_datapoint']['mandatory']\n",
    "        opt = json_data['list_of_datapoint']['optional']\n",
    "\n",
    "        # Combines data points and saves them as preferred_datafields\n",
    "        preferred_datafields = man + opt\n",
    "\n",
    "        if preferred_datafields != '':\n",
    "            df_data = df_data[preferred_datafields]\n",
    "        return df_data\n",
    "\n",
    "    # Checks missing values\n",
    "    @staticmethod\n",
    "    def checkMissingValues(df_clean):\n",
    "        percent_missing = df_clean.isnull().sum() * 100 / len(df_clean)\n",
    "        return percent_missing\n",
    "\n",
    "    # Get name of the columns with missing values\n",
    "    @staticmethod\n",
    "    def getMissingValueColumns(df_clean, required_percentage):\n",
    "        columns = df_clean.columns\n",
    "        percent = Functions.checkMissingValues(df_clean)\n",
    "        percent_missing_val = pd.DataFrame({'column_name': columns, 'percent_missing': percent}).sort_values(\n",
    "            by='percent_missing', ascending=True)\n",
    "        # print(percent_missing_val)\n",
    "\n",
    "        # if \"percent_missing\" >50%, then noting as 'higher missing values'\n",
    "        missing_val_columns = percent_missing_val.loc[\n",
    "            percent_missing_val['percent_missing'] > required_percentage].index.tolist()\n",
    "        # print(\"Columns with more than 50% of the data missing: \", missing_val_columns)\n",
    "\n",
    "        return missing_val_columns\n",
    "\n",
    "    @staticmethod\n",
    "    def missingValueImputation(df_clean):\n",
    "        df_clean['Position'] = df_clean.groupby('Department').Position.transform(lambda x: x.fillna(x.mode()[0]))\n",
    "        return df_clean\n",
    "\n",
    "    @staticmethod\n",
    "    def dropColumns(df_clean, column):\n",
    "        df_clean = df_clean.drop(column, axis=1)\n",
    "        return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "\n",
    "    CSV_PATH = 'data/'\n",
    "    DATE_FORMAT = '%Y-%m-%d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleansing:\n",
    "    # Converts into DATE_FORMAT\n",
    "    @staticmethod\n",
    "    def convertDateTime(df_data, column):\n",
    "        df_data[column] = df_data[column].apply(\n",
    "            lambda x: np.nan if (x == 'null' or x == '0' or x == 0 or x is None or x == np.nan) else datetime.utcfromtimestamp(\n",
    "                int(x)).strftime(Parameters.DATE_FORMAT))\n",
    "        # print(df_data)\n",
    "        return df_data\n",
    "\n",
    "    # Data Cleaning\n",
    "    @staticmethod\n",
    "    def dataCleaning(df_data):\n",
    "        # check how long the dataframe is\n",
    "        # df = len(df_data)\n",
    "        # print(df)\n",
    "\n",
    "        # number of columns\n",
    "        # df = df_data.columns\n",
    "        # df = df.size\n",
    "        # print(df)\n",
    "\n",
    "        # number of Nan values in each column\n",
    "        # for column in df_data.columns:\n",
    "        #     df = df_data[column].isnull()\n",
    "        #     print(column, \" - \", df)\n",
    "\n",
    "        # check if the values in the given column in unique\n",
    "        # df_data['Employee_ID'].is_unique\n",
    "        # print('Employee ID is unique')\n",
    "\n",
    "        # Converts null and 0 into np.nan\n",
    "        for column in df_data.columns:\n",
    "            if column in ('Employee_ID', 'Education', 'Job_Satisfaction', 'Marital_Status'):\n",
    "                pass\n",
    "            elif column == 'Years_At_Company':\n",
    "                df_data[column] = df_data[column].fillna(0)\n",
    "                if column in 'Years_At_Company':\n",
    "                    df_data = DataCleansing.convertDateTime(df_data, 'Years_At_Company')\n",
    "            elif column == 'Start_Date':\n",
    "                df_data[column] = df_data[column].fillna(0)\n",
    "                if column in 'Start_Date':\n",
    "                    df_data = DataCleansing.convertDateTime(df_data, 'Start_Date')\n",
    "            else:\n",
    "                df_data[column] = df_data[column].apply(lambda x: np.nan if x == 'null' or x == '0' or x == 0 or\n",
    "                                                                            x == None else x)\n",
    "        return df_data\n",
    "\n",
    "    # Imputation\n",
    "    @staticmethod\n",
    "    def imputeOnce(df_clean):\n",
    "        # data = self.loadCsvFile('HR-Employee-Dataset.csv')\n",
    "        # dataframe = self.replaceNaN(data)\n",
    "        # print(df_clean.columns)\n",
    "        imputer = IterativeImputer()\n",
    "        imputer.fit(df_clean)\n",
    "        Xtrans = imputer.transform(df_clean)\n",
    "        imputed_df = pd.DataFrame(Xtrans, columns=df_clean.columns)\n",
    "        # print(imputed_df)\n",
    "        # print('Missing after impute: %d' % sum(isnan(Xtrans).flatten()))\n",
    "        # Utilities.exportCsvFile(imputed_df, file_name='missing_val_imputed_dataframe')\n",
    "        return imputed_df\n",
    "\n",
    "    @staticmethod\n",
    "    def labelEncoding(df_data):\n",
    "        # get categorical columns\n",
    "        labelEncoding = preprocessing.LabelEncoder()\n",
    "        catCols = df_data.select_dtypes(\"object\").columns\n",
    "        # print(catCols)\n",
    "        for column in catCols:\n",
    "            df_data[column] = labelEncoding.fit_transform(df_data[column])\n",
    "        return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    @staticmethod\n",
    "    def X_prepare(df_clean):\n",
    "        # print(df_clean.columns)\n",
    "        X_label = df_clean[['Position', 'Department', 'Gender', 'Education_Field']]\n",
    "        # X_label_columns = X_label.columns\n",
    "        X_number = df_clean[['Age', 'Education',\n",
    "                             'Job_Satisfaction', 'Hourly_Rate', 'Monthly_Rate',\n",
    "                             'Monthly_Income',\n",
    "                             'Percent_Salary_Hike', 'Performance_Rating', 'Total_Working_Years',\n",
    "                             'Work_Life_Balance', 'Start_Date'\n",
    "                             ]]\n",
    "        encoder = OneHotEncoder()\n",
    "        X_label_onehot = encoder.fit_transform(X_label).toarray()\n",
    "        scaler = StandardScaler()\n",
    "        X_number_standard = scaler.fit_transform(X_number)\n",
    "        # X_number_pd = pd.DataFrame(X_number_standard, columns=X_number.columns)\n",
    "        X = np.concatenate((X_number_standard, X_label_onehot), axis=1)\n",
    "        # print('X-->', pd.DataFrame(X))\n",
    "        # print('X.shape: ', X.shape)\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def y_prepare(y_sampling):\n",
    "        y = y_sampling[:, np.newaxis]\n",
    "        encoder = OneHotEncoder()\n",
    "        y = encoder.fit_transform(y).toarray()\n",
    "        # print(y.shape)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def imbalanceAndModelTraining(df_clean):\n",
    "        y_sampling = df_clean['is_active']\n",
    "        x_sampling = df_clean.drop(columns=['is_active'])\n",
    "        counter = Counter(y_sampling)\n",
    "        # print(counter)\n",
    "\n",
    "        if df_clean.isnull().sum().sum() == 0:\n",
    "            # SMOTE\n",
    "            oversample = SMOTE(random_state=42, k_neighbors=1)\n",
    "            x_smote, y_smote = oversample.fit_sample(x_sampling, y_sampling)\n",
    "            counter = Counter(y_smote)\n",
    "            # print(counter)\n",
    "            X = Training.X_prepare(df_clean)\n",
    "            y = Training.y_prepare(y_sampling)\n",
    "            # Splitting the dataset into the Training set and Test set\n",
    "            x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "            # feature importance\n",
    "            rf_params = {\n",
    "                'n_jobs': -1,\n",
    "                'n_estimators': 1000,\n",
    "                #     'warm_start': True,\n",
    "                'max_features': 0.3,\n",
    "                'max_depth': 4,\n",
    "                'min_samples_leaf': 2,\n",
    "                'max_features': 'sqrt',\n",
    "                'random_state': 0,\n",
    "                'verbose': 0\n",
    "            }\n",
    "            x_train_fea, x_test_fea, y_train_fea, y_test_fea = train_test_split(x_sampling, y_sampling, test_size=0.20,\n",
    "                                                                                random_state=0)\n",
    "            rfc = RandomForestClassifier(**rf_params)\n",
    "            rfc.fit(x_train_fea, y_train_fea)\n",
    "            importances = rfc.feature_importances_\n",
    "            importances = pd.DataFrame({'feature': x_sampling, 'importance': 100 * np.round(importances, 3)})\n",
    "            importances = importances.sort_values('importance', ascending=False).set_index('feature')\n",
    "\n",
    "            print('-0-----------------------', importances)\n",
    "\n",
    "            # Model Training\n",
    "            net = tflearn.input_data(shape=[None, X.shape[1]])\n",
    "            net = tflearn.fully_connected(net, 6, activation='relu')\n",
    "            net = tflearn.fully_connected(net, 6, activation='relu')\n",
    "            net = tflearn.fully_connected(net, 6, activation='relu')\n",
    "            net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "            net = tflearn.regression(net)\n",
    "            model = tflearn.DNN(net)\n",
    "            model.fit(x_train, y_train, n_epoch=120, batch_size=32, show_metric=True)\n",
    "\n",
    "            y_pred = model.predict(X)\n",
    "\n",
    "            df_turnover = pd.DataFrame(\n",
    "                {'emp_identifier': df_clean.Employee_ID, 'turnover_percent': 100 * y_pred[:, 0]})\n",
    "            print(df_turnover)\n",
    "\n",
    "            score_test = model.evaluate(x_test, y_test)\n",
    "            print('X_test, y_test Accuracy: %0.4f%%' % (score_test[0] * 100))\n",
    "\n",
    "        return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4439  | total loss: \u001b[1m\u001b[32m0.34474\u001b[0m\u001b[0m | time: 0.043s\n",
      "| Adam | epoch: 120 | loss: 0.34474 - acc: 0.8699 -- iter: 1152/1176\n",
      "Training Step: 4440  | total loss: \u001b[1m\u001b[32m0.36853\u001b[0m\u001b[0m | time: 0.044s\n",
      "| Adam | epoch: 120 | loss: 0.36853 - acc: 0.8579 -- iter: 1176/1176\n",
      "--\n",
      "      emp_identifier  turnover_percent\n",
      "0             1001.0         70.591393\n",
      "1             1002.0         86.407455\n",
      "2             1003.0         79.107384\n",
      "3             1004.0         81.797485\n",
      "4             1005.0         85.845879\n",
      "...              ...               ...\n",
      "1465          2466.0         92.981216\n",
      "1466          2467.0         94.278839\n",
      "1467          2468.0         98.448738\n",
      "1468          2469.0         92.799294\n",
      "1469          2470.0         84.638916\n",
      "\n",
      "[1470 rows x 2 columns]\n",
      "X_test, y_test Accuracy: 83.3333%\n"
     ]
    }
   ],
   "source": [
    "func = Functions()\n",
    "class MainNew:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"MLOps\").getOrCreate()\n",
    "\n",
    "    # Function to run all the functions\n",
    "    def run(self):\n",
    "        raw_data = func.loadCsvFile(spark=self.spark, file_name='HR-Employee-Dataset-Large.csv')\n",
    "        df_clean = DataCleansing.dataCleaning(raw_data)\n",
    "        qualify_data = func.checkMissingValues(df_clean)\n",
    "        unqualified_columns = func.getMissingValueColumns(df_clean, 50)\n",
    "        df_clean = func.dropColumns(df_clean, unqualified_columns)\n",
    "        df_clean = func.missingValueImputation(df_clean)\n",
    "        df_encode = DataCleansing.labelEncoding(df_clean)\n",
    "        df_clean = DataCleansing.imputeOnce(df_encode)\n",
    "        df_clean = Training.imbalanceAndModelTraining(df_clean)\n",
    "        # # test.printCSV(df_clean)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mainNew = MainNew()\n",
    "    mainNew.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
